{
  "meta-llama/Llama-2-7b-hf": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "Llama 2",
    "size_category": "7B",
    "release_date": "2023-07",
    "context_length": 4096
  },
  "meta-llama/Llama-2-13b-hf": {
    "vocab_size": 32000,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 13824,
    "num_hidden_layers": 40,
    "model_type": "llama",
    "family": "Llama 2",
    "size_category": "13B",
    "release_date": "2023-07",
    "context_length": 4096
  },
  "meta-llama/Llama-2-70b-hf": {
    "vocab_size": 32000,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 28672,
    "num_hidden_layers": 80,
    "model_type": "llama",
    "family": "Llama 2",
    "size_category": "70B",
    "release_date": "2023-07",
    "context_length": 4096
  },
  "meta-llama/Meta-Llama-3-8B": {
    "vocab_size": 128256,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "Llama 3",
    "size_category": "8B",
    "release_date": "2024-04",
    "context_length": 8192
  },
  "meta-llama/Meta-Llama-3-70B": {
    "vocab_size": 128256,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 28672,
    "num_hidden_layers": 80,
    "model_type": "llama",
    "family": "Llama 3",
    "size_category": "70B",
    "release_date": "2024-04",
    "context_length": 8192
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "Llama 3.1",
    "size_category": "8B",
    "release_date": "2024-07",
    "context_length": 131072
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 28672,
    "num_hidden_layers": 80,
    "model_type": "llama",
    "family": "Llama 3.1",
    "size_category": "70B",
    "release_date": "2024-07",
    "context_length": 131072
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 16384,
    "num_attention_heads": 128,
    "intermediate_size": 53248,
    "num_hidden_layers": 126,
    "model_type": "llama",
    "family": "Llama 3.1",
    "size_category": "405B",
    "release_date": "2024-07",
    "context_length": 131072
  },
  "meta-llama/Llama-3.2-1B-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 2048,
    "num_attention_heads": 32,
    "intermediate_size": 8192,
    "num_hidden_layers": 16,
    "model_type": "llama",
    "family": "Llama 3.2",
    "size_category": "1B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "meta-llama/Llama-3.2-3B-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 3072,
    "num_attention_heads": 24,
    "intermediate_size": 8192,
    "num_hidden_layers": 28,
    "model_type": "llama",
    "family": "Llama 3.2",
    "size_category": "3B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "meta-llama/Llama-3.2-11B-Vision-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "Llama 3.2",
    "size_category": "11B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "meta-llama/Llama-3.2-90B-Vision-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 28672,
    "num_hidden_layers": 80,
    "model_type": "llama",
    "family": "Llama 3.2",
    "size_category": "90B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "meta-llama/Llama-3.3-70B-Instruct": {
    "vocab_size": 128256,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 28672,
    "num_hidden_layers": 80,
    "model_type": "llama",
    "family": "Llama 3.3",
    "size_category": "70B",
    "release_date": "2024-12",
    "context_length": 131072
  },
  "mistralai/Mistral-7B-v0.1": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "mistral",
    "family": "Mistral",
    "size_category": "7B",
    "release_date": "2023-09",
    "context_length": 32768
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "vocab_size": 32768,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "mistral",
    "family": "Mistral",
    "size_category": "7B",
    "release_date": "2024-05",
    "context_length": 32768
  },
  "mistralai/Mixtral-8x7B-v0.1": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "mixtral",
    "family": "Mixtral",
    "size_category": "8x7B",
    "release_date": "2023-12",
    "context_length": 32768
  },
  "mistralai/Mixtral-8x22B-v0.1": {
    "vocab_size": 32768,
    "hidden_size": 6144,
    "num_attention_heads": 48,
    "intermediate_size": 16384,
    "num_hidden_layers": 56,
    "model_type": "mixtral",
    "family": "Mixtral",
    "size_category": "8x22B",
    "release_date": "2024-04",
    "context_length": 65536
  },
  "mistralai/Mistral-Nemo-Instruct-2407": {
    "vocab_size": 131072,
    "hidden_size": 5120,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 40,
    "model_type": "mistral",
    "family": "Mistral Nemo",
    "size_category": "12B",
    "release_date": "2024-07",
    "context_length": 131072
  },
  "mistralai/Mistral-Large-Instruct-2411": {
    "vocab_size": 131072,
    "hidden_size": 12288,
    "num_attention_heads": 96,
    "intermediate_size": 28672,
    "num_hidden_layers": 88,
    "model_type": "mistral",
    "family": "Mistral Large",
    "size_category": "123B",
    "release_date": "2024-11",
    "context_length": 131072
  },
  "microsoft/Phi-3-mini-4k-instruct": {
    "vocab_size": 32064,
    "hidden_size": 3072,
    "num_attention_heads": 32,
    "intermediate_size": 8192,
    "num_hidden_layers": 32,
    "model_type": "phi3",
    "family": "Phi-3",
    "size_category": "3.8B",
    "release_date": "2024-04",
    "context_length": 4096
  },
  "microsoft/Phi-3-small-8k-instruct": {
    "vocab_size": 100352,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "phi3",
    "family": "Phi-3",
    "size_category": "7B",
    "release_date": "2024-05",
    "context_length": 8192
  },
  "microsoft/Phi-3-medium-4k-instruct": {
    "vocab_size": 32064,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 17920,
    "num_hidden_layers": 40,
    "model_type": "phi3",
    "family": "Phi-3",
    "size_category": "14B",
    "release_date": "2024-05",
    "context_length": 4096
  },
  "microsoft/Phi-3.5-mini-instruct": {
    "vocab_size": 32064,
    "hidden_size": 3072,
    "num_attention_heads": 32,
    "intermediate_size": 8192,
    "num_hidden_layers": 32,
    "model_type": "phi3",
    "family": "Phi-3.5",
    "size_category": "3.8B",
    "release_date": "2024-08",
    "context_length": 131072
  },
  "google/gemma-2b": {
    "vocab_size": 256000,
    "hidden_size": 2048,
    "num_attention_heads": 8,
    "intermediate_size": 16384,
    "num_hidden_layers": 18,
    "model_type": "gemma",
    "family": "Gemma",
    "size_category": "2B",
    "release_date": "2024-02",
    "context_length": 8192
  },
  "google/gemma-7b": {
    "vocab_size": 256000,
    "hidden_size": 3072,
    "num_attention_heads": 16,
    "intermediate_size": 24576,
    "num_hidden_layers": 28,
    "model_type": "gemma",
    "family": "Gemma",
    "size_category": "7B",
    "release_date": "2024-02",
    "context_length": 8192
  },
  "google/gemma-2-2b": {
    "vocab_size": 256000,
    "hidden_size": 2304,
    "num_attention_heads": 8,
    "intermediate_size": 9216,
    "num_hidden_layers": 26,
    "model_type": "gemma2",
    "family": "Gemma 2",
    "size_category": "2B",
    "release_date": "2024-06",
    "context_length": 8192
  },
  "google/gemma-2-9b": {
    "vocab_size": 256000,
    "hidden_size": 3584,
    "num_attention_heads": 16,
    "intermediate_size": 14336,
    "num_hidden_layers": 42,
    "model_type": "gemma2",
    "family": "Gemma 2",
    "size_category": "9B",
    "release_date": "2024-06",
    "context_length": 8192
  },
  "google/gemma-2-27b": {
    "vocab_size": 256000,
    "hidden_size": 4608,
    "num_attention_heads": 32,
    "intermediate_size": 36864,
    "num_hidden_layers": 46,
    "model_type": "gemma2",
    "family": "Gemma 2",
    "size_category": "27B",
    "release_date": "2024-06",
    "context_length": 8192
  },
  "Qwen/Qwen2-0.5B": {
    "vocab_size": 151936,
    "hidden_size": 896,
    "num_attention_heads": 14,
    "intermediate_size": 4864,
    "num_hidden_layers": 24,
    "model_type": "qwen2",
    "family": "Qwen2",
    "size_category": "0.5B",
    "release_date": "2024-06",
    "context_length": 32768
  },
  "Qwen/Qwen2-1.5B": {
    "vocab_size": 151936,
    "hidden_size": 1536,
    "num_attention_heads": 12,
    "intermediate_size": 8960,
    "num_hidden_layers": 28,
    "model_type": "qwen2",
    "family": "Qwen2",
    "size_category": "1.5B",
    "release_date": "2024-06",
    "context_length": 32768
  },
  "Qwen/Qwen2-7B": {
    "vocab_size": 152064,
    "hidden_size": 3584,
    "num_attention_heads": 28,
    "intermediate_size": 18944,
    "num_hidden_layers": 28,
    "model_type": "qwen2",
    "family": "Qwen2",
    "size_category": "7B",
    "release_date": "2024-06",
    "context_length": 131072
  },
  "Qwen/Qwen2-72B": {
    "vocab_size": 152064,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 29568,
    "num_hidden_layers": 80,
    "model_type": "qwen2",
    "family": "Qwen2",
    "size_category": "72B",
    "release_date": "2024-06",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-0.5B": {
    "vocab_size": 151936,
    "hidden_size": 896,
    "num_attention_heads": 14,
    "intermediate_size": 4864,
    "num_hidden_layers": 24,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "0.5B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-1.5B": {
    "vocab_size": 151936,
    "hidden_size": 1536,
    "num_attention_heads": 12,
    "intermediate_size": 8960,
    "num_hidden_layers": 28,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "1.5B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-3B": {
    "vocab_size": 151936,
    "hidden_size": 2048,
    "num_attention_heads": 16,
    "intermediate_size": 11008,
    "num_hidden_layers": 36,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "3B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-7B": {
    "vocab_size": 152064,
    "hidden_size": 3584,
    "num_attention_heads": 28,
    "intermediate_size": 18944,
    "num_hidden_layers": 28,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "7B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-14B": {
    "vocab_size": 152064,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 13824,
    "num_hidden_layers": 48,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "14B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-32B": {
    "vocab_size": 152064,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 27648,
    "num_hidden_layers": 64,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "32B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "Qwen/Qwen2.5-72B": {
    "vocab_size": 152064,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 29568,
    "num_hidden_layers": 80,
    "model_type": "qwen2",
    "family": "Qwen2.5",
    "size_category": "72B",
    "release_date": "2024-09",
    "context_length": 131072
  },
  "deepseek-ai/deepseek-coder-6.7b-base": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "DeepSeek Coder",
    "size_category": "6.7B",
    "release_date": "2023-11",
    "context_length": 16384
  },
  "deepseek-ai/deepseek-coder-33b-base": {
    "vocab_size": 32000,
    "hidden_size": 7168,
    "num_attention_heads": 56,
    "intermediate_size": 18944,
    "num_hidden_layers": 62,
    "model_type": "llama",
    "family": "DeepSeek Coder",
    "size_category": "33B",
    "release_date": "2023-11",
    "context_length": 16384
  },
  "deepseek-ai/DeepSeek-V2-Lite": {
    "vocab_size": 102400,
    "hidden_size": 2048,
    "num_attention_heads": 16,
    "intermediate_size": 10944,
    "num_hidden_layers": 27,
    "model_type": "deepseek_v2",
    "family": "DeepSeek V2",
    "size_category": "16B",
    "release_date": "2024-05",
    "context_length": 32768
  },
  "deepseek-ai/DeepSeek-V2": {
    "vocab_size": 102400,
    "hidden_size": 5120,
    "num_attention_heads": 128,
    "intermediate_size": 12288,
    "num_hidden_layers": 60,
    "model_type": "deepseek_v2",
    "family": "DeepSeek V2",
    "size_category": "236B",
    "release_date": "2024-05",
    "context_length": 131072
  },
  "deepseek-ai/DeepSeek-Coder-V2-Lite-Base": {
    "vocab_size": 102400,
    "hidden_size": 2048,
    "num_attention_heads": 16,
    "intermediate_size": 10944,
    "num_hidden_layers": 27,
    "model_type": "deepseek_v2",
    "family": "DeepSeek Coder V2",
    "size_category": "16B",
    "release_date": "2024-06",
    "context_length": 163840
  },
  "HuggingFaceH4/zephyr-7b-beta": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "mistral",
    "family": "Zephyr",
    "size_category": "7B",
    "release_date": "2023-10",
    "context_length": 32768
  },
  "teknium/OpenHermes-2.5-Mistral-7B": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "mistral",
    "family": "OpenHermes",
    "size_category": "7B",
    "release_date": "2023-11",
    "context_length": 8192
  },
  "openchat/openchat-3.5-0106": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "mistral",
    "family": "OpenChat",
    "size_category": "7B",
    "release_date": "2024-01",
    "context_length": 8192
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "mixtral",
    "family": "Nous Hermes",
    "size_category": "8x7B",
    "release_date": "2024-01",
    "context_length": 32768
  },
  "garage-bAInd/Platypus2-70B-instruct": {
    "vocab_size": 32000,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 28672,
    "num_hidden_layers": 80,
    "model_type": "llama",
    "family": "Platypus",
    "size_category": "70B",
    "release_date": "2023-08",
    "context_length": 4096
  },
  "WizardLM/WizardCoder-15B-V1.0": {
    "vocab_size": 49152,
    "hidden_size": 6144,
    "num_attention_heads": 48,
    "intermediate_size": 24576,
    "num_hidden_layers": 40,
    "model_type": "starcoder",
    "family": "WizardCoder",
    "size_category": "15B",
    "release_date": "2023-06",
    "context_length": 8192
  },
  "codellama/CodeLlama-7b-hf": {
    "vocab_size": 32016,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "Code Llama",
    "size_category": "7B",
    "release_date": "2023-08",
    "context_length": 16384
  },
  "codellama/CodeLlama-13b-hf": {
    "vocab_size": 32016,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 13824,
    "num_hidden_layers": 40,
    "model_type": "llama",
    "family": "Code Llama",
    "size_category": "13B",
    "release_date": "2023-08",
    "context_length": 16384
  },
  "codellama/CodeLlama-34b-hf": {
    "vocab_size": 32016,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 22016,
    "num_hidden_layers": 48,
    "model_type": "llama",
    "family": "Code Llama",
    "size_category": "34B",
    "release_date": "2023-08",
    "context_length": 16384
  },
  "bigscience/bloom-3b": {
    "vocab_size": 250880,
    "hidden_size": 2560,
    "num_attention_heads": 32,
    "intermediate_size": 10240,
    "num_hidden_layers": 30,
    "model_type": "bloom",
    "family": "BLOOM",
    "size_category": "3B",
    "release_date": "2022-07",
    "context_length": 2048
  },
  "bigscience/bloom-7b1": {
    "vocab_size": 250880,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 16384,
    "num_hidden_layers": 30,
    "model_type": "bloom",
    "family": "BLOOM",
    "size_category": "7B",
    "release_date": "2022-07",
    "context_length": 2048
  },
  "bigscience/bloomz-7b1": {
    "vocab_size": 250880,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 16384,
    "num_hidden_layers": 30,
    "model_type": "bloom",
    "family": "BLOOMZ",
    "size_category": "7B",
    "release_date": "2022-11",
    "context_length": 2048
  },
  "EleutherAI/gpt-neo-2.7B": {
    "vocab_size": 50257,
    "hidden_size": 2560,
    "num_attention_heads": 20,
    "intermediate_size": 10240,
    "num_hidden_layers": 32,
    "model_type": "gpt_neo",
    "family": "GPT-Neo",
    "size_category": "2.7B",
    "release_date": "2021-03",
    "context_length": 2048
  },
  "EleutherAI/gpt-j-6b": {
    "vocab_size": 50400,
    "hidden_size": 4096,
    "num_attention_heads": 16,
    "intermediate_size": 16384,
    "num_hidden_layers": 28,
    "model_type": "gptj",
    "family": "GPT-J",
    "size_category": "6B",
    "release_date": "2021-06",
    "context_length": 2048
  },
  "EleutherAI/gpt-neox-20b": {
    "vocab_size": 50432,
    "hidden_size": 6144,
    "num_attention_heads": 48,
    "intermediate_size": 24576,
    "num_hidden_layers": 44,
    "model_type": "gpt_neox",
    "family": "GPT-NeoX",
    "size_category": "20B",
    "release_date": "2022-04",
    "context_length": 2048
  },
  "tiiuae/falcon-7b": {
    "vocab_size": 65024,
    "hidden_size": 4544,
    "num_attention_heads": 71,
    "intermediate_size": 18176,
    "num_hidden_layers": 32,
    "model_type": "falcon",
    "family": "Falcon",
    "size_category": "7B",
    "release_date": "2023-05",
    "context_length": 2048
  },
  "tiiuae/falcon-40b": {
    "vocab_size": 65024,
    "hidden_size": 8192,
    "num_attention_heads": 128,
    "intermediate_size": 32768,
    "num_hidden_layers": 60,
    "model_type": "falcon",
    "family": "Falcon",
    "size_category": "40B",
    "release_date": "2023-05",
    "context_length": 2048
  },
  "mosaicml/mpt-7b": {
    "vocab_size": 50432,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 16384,
    "num_hidden_layers": 32,
    "model_type": "mpt",
    "family": "MPT",
    "size_category": "7B",
    "release_date": "2023-05",
    "context_length": 2048
  },
  "mosaicml/mpt-30b": {
    "vocab_size": 50432,
    "hidden_size": 7168,
    "num_attention_heads": 56,
    "intermediate_size": 28672,
    "num_hidden_layers": 48,
    "model_type": "mpt",
    "family": "MPT",
    "size_category": "30B",
    "release_date": "2023-06",
    "context_length": 8192
  },
  "stabilityai/stablelm-3b-4e1t": {
    "vocab_size": 50304,
    "hidden_size": 2560,
    "num_attention_heads": 32,
    "intermediate_size": 6912,
    "num_hidden_layers": 32,
    "model_type": "stablelm",
    "family": "StableLM",
    "size_category": "3B",
    "release_date": "2023-04",
    "context_length": 4096
  },
  "togethercomputer/RedPajama-INCITE-7B-Base": {
    "vocab_size": 50432,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "llama",
    "family": "RedPajama",
    "size_category": "7B",
    "release_date": "2023-05",
    "context_length": 2048
  },
  "01-ai/Yi-6B": {
    "vocab_size": 64000,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "yi",
    "family": "Yi",
    "size_category": "6B",
    "release_date": "2023-11",
    "context_length": 4096
  },
  "01-ai/Yi-34B": {
    "vocab_size": 64000,
    "hidden_size": 7168,
    "num_attention_heads": 56,
    "intermediate_size": 20480,
    "num_hidden_layers": 60,
    "model_type": "yi",
    "family": "Yi",
    "size_category": "34B",
    "release_date": "2023-11",
    "context_length": 4096
  },
  "xverse/XVERSE-7B": {
    "vocab_size": 100278,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "xverse",
    "family": "XVERSE",
    "size_category": "7B",
    "release_date": "2023-09",
    "context_length": 8192
  },
  "xverse/XVERSE-13B": {
    "vocab_size": 100278,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 13696,
    "num_hidden_layers": 40,
    "model_type": "xverse",
    "family": "XVERSE",
    "size_category": "13B",
    "release_date": "2023-08",
    "context_length": 8192
  },
  "xverse/XVERSE-65B": {
    "vocab_size": 100278,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "intermediate_size": 22016,
    "num_hidden_layers": 80,
    "model_type": "xverse",
    "family": "XVERSE",
    "size_category": "65B",
    "release_date": "2023-09",
    "context_length": 16384
  },
  "baichuan-inc/Baichuan2-7B-Base": {
    "vocab_size": 125696,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "model_type": "baichuan",
    "family": "Baichuan2",
    "size_category": "7B",
    "release_date": "2023-09",
    "context_length": 4096
  },
  "baichuan-inc/Baichuan2-13B-Base": {
    "vocab_size": 125696,
    "hidden_size": 5120,
    "num_attention_heads": 40,
    "intermediate_size": 13696,
    "num_hidden_layers": 40,
    "model_type": "baichuan",
    "family": "Baichuan2",
    "size_category": "13B",
    "release_date": "2023-09",
    "context_length": 4096
  },
  "internlm/internlm2-7b": {
    "vocab_size": 92544,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_hidden_layers": 32,
    "model_type": "internlm2",
    "family": "InternLM2",
    "size_category": "7B",
    "release_date": "2024-01",
    "context_length": 32768
  },
  "internlm/internlm2-20b": {
    "vocab_size": 92544,
    "hidden_size": 6144,
    "num_attention_heads": 48,
    "intermediate_size": 16384,
    "num_hidden_layers": 48,
    "model_type": "internlm2",
    "family": "InternLM2",
    "size_category": "20B",
    "release_date": "2024-01",
    "context_length": 32768
  }
}